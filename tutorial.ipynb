{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: pymilvus[model]\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet pymilvus[model] langchain-milvus langchain-openai\n",
    "%pip install -qU langchain_community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTION_URI = \"http://localhost:19530\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_embedding_func = OllamaLLM(model=\"moondream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2303.05510v1.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Ollama Setup\n",
    "def setup_ollama_pdf_vectorstore(file_path, embedding_model=\"moondream\"):\n",
    "    try:\n",
    "        # 1. Load PDF\n",
    "        loader = PyMuPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "\n",
    "        # 2. Text Splitting (Optional but recommended)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        splits = text_splitter.split_documents(docs)\n",
    "\n",
    "        # 3. Ollama Embeddings\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            model=embedding_model,  # You can change this to other Ollama models\n",
    "            num_gpu=16  # Optional: specify GPU usage\n",
    "        )\n",
    "\n",
    "        # 4. Create Chroma Vector Store\n",
    "        vector_store = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embeddings,\n",
    "            collection_name=\"ollama_pdf_collection\"\n",
    "        )\n",
    "\n",
    "        return vector_store\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "files = os.listdir(\"papers\")\n",
    "print(files)\n",
    "\n",
    "vector_store = setup_ollama_pdf_vectorstore(f\"papers/{files[0]}\")\n",
    "print(f\"Vector store for {files[0]} created successfully\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "Published as a conference paper at ICLR 2023\n",
      "APPENDIX\n",
      "In this appendix, we supplement the main paper by providing more thorough empirical evaluations to\n",
      "back up our claims and more detailed descriptions of the algorithms to help readers better understand\n",
      "our paper.\n",
      "This appendix is organized as follows.\n",
      "• In Sec. A, we provide more comprehensive results of our algorithm and the baseline algorithms.\n",
      "We also include the license information of the datasets we use.\n",
      "• In Sec. B, we consider the scenario where test cases are not provided. We evaluate our PG-TD\n",
      "algorithm using automatically-generated test cases.\n",
      "• In Sec. C, we provide empirical evidence for our claims in the discussion section that our\n",
      "algorithm is versatile and can be used to optimize different code generation objectives other\n",
      "than the pass rate. We consider the objectives of generating concise codes and generating\n",
      "codes with more comments.\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "the Transformer beam search algorithm with tree search.\n",
      "Consider the example in Fig. 3, which shows two iter-\n",
      "ations of PG-TD. In the evaluation step of the t-th iter-\n",
      "ation, the Transformer beam search algorithm implicitly\n",
      "builds a tree to ﬁnd the most likely sequences within a\n",
      "beam (Fig. 3 (left)). Because we only keep b partial pro-\n",
      "grams in the beam, it is a tree where only b nodes with the\n",
      "highest likelihood are expanded at each level (in the illus-\n",
      "tration, b = 2). Other nodes are dropped and no longer\n",
      "considered by the beam search algorithm. In the (t+1)-st\n",
      "iteration, if the tree search algorithm selects “a,”, such a\n",
      "state is already visited in the Transformer beam search in\n",
      "the t-th iteration. When the algorithm needs to ﬁnd the\n",
      "top-k most likely next tokens, such information is already obtained in the t-th iteration and can be\n",
      "reused without recomputation. In our implementation, we cache the tree structure generated by the\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "set of CodeContests (165 problems) (Li et al., 2022). We use the same metrics as in Hendrycks et al.\n",
      "(2021), which are pass rates and strict accuracies on the private test cases. Speciﬁcally, the pass\n",
      "rate is the average percentage of the private test cases that the generated programs pass over all the\n",
      "problems. The strict accuracy is the percentage of the problems where the generated programs pass\n",
      "all the private test cases.\n",
      "Effectiveness of PG-TD. To make a fair comparison, we evaluate the best programs found by the\n",
      "algorithms when they use the same number of Transformer generations. Speciﬁcally, the num-\n",
      "ber of Transformer generations is the number of function calls of BEAM SEARCH in PG-TD and\n",
      "SMCG-TD (when no cached sequences are found and used in sequence caching), and the number\n",
      "of sampling function calls in S+F.\n",
      "As shown in Table 1, PG-TD consistently outperforms all the other baselines on all the datasets\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "Transformer model, we run beam search to generate solutions on the test set to see if it has a better\n",
      "performance. We use the ﬁrst 500 problems in the interview-level problems in APPS test set for\n",
      "validation and the introductory-level problems in APPS test set for testing. The results are reported\n",
      "in Table 3. After ﬁnetuning with PG-TD-generated solutions, we see improvement in both pass rate\n",
      "and strict accuracy. More details are in Sec. D.4.\n",
      "Methods\n",
      "Code ↓\n",
      "Comment ↑\n",
      "Pass ↑\n",
      "length\n",
      "number\n",
      "rate\n",
      "Default\n",
      "248.42\n",
      "0.68\n",
      "23.14\n",
      "Length Penalty\n",
      "190.73\n",
      "-\n",
      "22.82\n",
      "Comment Encouragement\n",
      "-\n",
      "3.11\n",
      "21.65\n",
      "Table 4: Performance of controllable code generation.\n",
      "Code length denotes the length of the generated code\n",
      "string and comment number denotes the number of code\n",
      "lines that contains comments.\n",
      "Optimizing other code generation\n",
      "objectives.\n",
      "Beyond the pass rate,\n",
      "we can make the algorithm versatile\n",
      "by using different reward functions.\n",
      "We consider two new objectives, code\n",
      "length penalty and comment encour-\n",
      "agement.\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "PG-TD (k = 2)\n",
      "PG-TD (k = 3)\n",
      "PG-TD (k = 4)\n",
      "Figure 7: Results of PG-TD (c = 4) with different k on the APPS introductory dataset, using the\n",
      "APPS GPT-2 Transformer model..\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "# of Transformer Generations\n",
      "22\n",
      "24\n",
      "26\n",
      "Pass Rate (%)\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "Computation Time (sec.)\n",
      "22\n",
      "24\n",
      "26\n",
      "Pass Rate (%)\n",
      "S + F (t = 0.6)\n",
      "S + F (t = 0.8)\n",
      "S + F (t = 1)\n",
      "Figure 8: Results of Sampling + Filtering with different temperatures on the APPS introductory\n",
      "dataset, using the APPS GPT-2 Transformer model.\n",
      "As we expect, PG-TD helps Codex generate better codes (Table 8). This further validates our claim\n",
      "that our method is model-agnostic and can help a pre-trained code generation Transformer generate\n",
      "better codes. Due to the limits of the OpenAI APIs, we are only able to test the algorithms on a\n",
      "subset of data. We also show a concrete example where PG-TD outperforms beam search in the\n",
      "following sections in Fig. 16.\n",
      "Using sampling-based tree search.\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "generation. RL has been used for both the training phase and the decoding phase of the Transformer\n",
      "for code generation. In the training phase, Le et al. (2022); Xu et al. (2019b) use an RL objective\n",
      "that optimizes the correctness of the generated programs instead of optimizing their similarity to the\n",
      "reference solutions. In the decoding phase, the Monte-Carlo tree search (MCTS) algorithm has been\n",
      "applied to search for high-quality codes. However, MCTS itself is unable to scale to larger domains.\n",
      "It is only used to generate domain-speciﬁc languages or for restricted programming synthesis tasks\n",
      "like assembly code generation (Xu et al., 2019a), Java bytecode translation (Lim & Yoo, 2016), and\n",
      "robot planning (Matulewicz, 2022). These tasks have a much smaller scale than generating Python\n",
      "codes in our work. Therefore, their frameworks do not require a large language model and do not\n",
      "need to address the challenge of integrating a planning algorithm with a large language model.\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "consider the example in Figure 13, where b = 2. Starting from “a,” (the left ﬁgure), suppose the\n",
      "beam search algorithm returns a sequence by searching two beams with the preﬁxes of “a,b” and\n",
      "“a,c”, respectively (the bold lines). In the next iteration (the right ﬁgure), to evaluate “a,b” using\n",
      "the beam search algorithm, we may not use the sequence found in the previous iteration, which only\n",
      "searches the subtree of “a,b” with one beam. In other words, we may ﬁnd a sequence with a higher\n",
      "likelihood by re-running the beam search algorithm with b = 2 starting from “a,b”.\n",
      "Although using sequence caching with b > 1 may underestimate the values of programs in the\n",
      "evaluation step in PG-TD, we still use sequence caching for all choices of b for the merits of com-\n",
      "putational saving.\n",
      "D.3\n",
      "BASELINE ALGORITHMS\n",
      "Sampling + Filtering.\n",
      "We have described the sampling algorithm in the main paper. The pseu-\n",
      "docode is shown in Alg. 2.\n",
      "Sequential Monte-Carlo-Guided Transformer Decoding.\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "Published as a conference paper at ICLR 2023\n",
      "Pass Rate (%)\n",
      "Strict Accuracy (%)\n",
      "APPS Intro.\n",
      "APPS Inter.\n",
      "APPS comp.\n",
      "CodeContests\n",
      "APPS Intro.\n",
      "APPS Inter.\n",
      "APPS comp.\n",
      "CodeContests\n",
      "APPS GPT-2\n",
      "Beam Search\n",
      "11.95\n",
      "9.55\n",
      "5.04\n",
      "5.10\n",
      "5.50\n",
      "2.10\n",
      "1.00\n",
      "0.00\n",
      "Sampling+Filtering\n",
      "25.19\n",
      "24.13\n",
      "11.92\n",
      "20.40\n",
      "13.80\n",
      "5.70\n",
      "2.30\n",
      "3.64\n",
      "SMCG-TD\n",
      "24.10\n",
      "21.98\n",
      "10.37\n",
      "17.47\n",
      "11.70\n",
      "5.50\n",
      "2.10\n",
      "4.24\n",
      "PG-TD (c = 4)\n",
      "26.70\n",
      "24.92\n",
      "12.89\n",
      "24.05\n",
      "13.10\n",
      "6.10\n",
      "3.10\n",
      "4.85\n",
      "APPS GPT-Neo\n",
      "Beam Search\n",
      "14.32\n",
      "9.80\n",
      "6.39\n",
      "5.73\n",
      "6.70\n",
      "2.00\n",
      "2.10\n",
      "0.00\n",
      "Sampling+Filtering\n",
      "27.71\n",
      "24.85\n",
      "12.55\n",
      "25.26\n",
      "15.50\n",
      "5.80\n",
      "3.00\n",
      "4.24\n",
      "SMCG-TD\n",
      "25.09\n",
      "20.34\n",
      "9.16\n",
      "15.44\n",
      "13.80\n",
      "5.10\n",
      "1.80\n",
      "3.03\n",
      "PG-TD (c = 4)\n",
      "29.27\n",
      "25.69\n",
      "13.55\n",
      "26.07\n",
      "15.50\n",
      "6.43\n",
      "3.50\n",
      "4.85\n",
      "Table 1: Results of PG-TD and other algorithms. The maximum number of Transformer generations\n",
      "for all algorithms is 256.\n",
      "200\n",
      "400\n",
      "600\n",
      "# of Transformer Generations\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "Pass Rate (%)\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "Computation Time (sec.)\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "Pass Rate (%)\n",
      "SMCG-TD\n",
      "Sampling + Filtering\n",
      "PG-TD (c = 2)\n",
      "PG-TD (c = 4)\n",
      "PG-TD (c = 6)\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "achieve higher performance compared with competing baseline methods; 2) it en-\n",
      "ables controllable code generation, such as concise codes and highly-commented\n",
      "codes by optimizing modiﬁed objective1.\n",
      "1\n",
      "INTRODUCTION\n",
      "Large language models like Transformer (Vaswani et al., 2017a) have shown successes in natural\n",
      "language processing, computer vision, and various other domains. Thanks to Transformer’s power\n",
      "on sequence modeling, it has been adopted for code generation (Wang et al., 2021; Ahmad et al.,\n",
      "2021) by treating programs as text sequences. Transformer has achieved signiﬁcant improvements\n",
      "on the benchmarking tasks of code translation (Roziere et al., 2022), code completion (Chen et al.,\n",
      "2021a), and solving coding challenge problems (Hendrycks et al., 2021). Recently, AlphaCode (Li\n",
      "et al., 2022) even achieved a competitive-level performance in programming competitions with the\n",
      "help of large Transformer models pre-trained on a large programming corpus.\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "33:10832–10842, 2020. 2\n",
      "Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\n",
      "models are unsupervised multitask learners. 2019. 2, 28\n",
      "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\n",
      "Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-\n",
      "text transformer. Journal of Machine Learning Research, 2020. URL http://jmlr.org/\n",
      "papers/v21/20-074.html. 2\n",
      "Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample. Unsupervised\n",
      "translation of programming languages. Advances in Neural Information Processing Systems, 33,\n",
      "2020. 2\n",
      "Baptiste Roziere, Jie M Zhang, Francois Charton, Mark Harman, Gabriel Synnaeve, and Guillaume\n",
      "Lample. Leveraging automated unit tests for unsupervised code translation. In International\n",
      "Conference on Learning Representations, 2022. 1, 3\n",
      "Thomas Scialom, Paul-Alexis Dray, Jacopo Staiano, Sylvain Lamprier, and Benjamin Piwowarski.\n"
     ]
    }
   ],
   "source": [
    "# Perform Similarity Search\n",
    "contexts = \"\"\n",
    "query = input(\"Enter your query: \")\n",
    "\n",
    "if vector_store:\n",
    "    results = vector_store.similarity_search(query, k=10)\n",
    "    \n",
    "    for doc in results:\n",
    "        print(\"\\n--- Relevant Document Excerpt ---\")\n",
    "        print(doc.page_content)\n",
    "        contexts = contexts + doc.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ollama Response ---\n",
      "The text appears to be a research paper or an academic article discussing the results of various machine learning models for code generation, specifically Transformer-based models.\n",
      "\n",
      "Here's a summary of the key findings:\n",
      "\n",
      "**Introduction**\n",
      "\n",
      "The authors introduce their work on using large language models like Transformer for code generation. They discuss how previous studies have achieved significant improvements in benchmarking tasks such as code translation, completion, and problem-solving.\n",
      "\n",
      "**Background and Related Work**\n",
      "\n",
      "The authors review existing research on large language models, including those by Vaswani et al. (2017a), Ahmad et al. (2021), Chen et al. (2021a), Hendrycks et al. (2021), Li et al. (2022), Raffel et al. (2020), Roziere et al. (2020), and Roziere et al. (2022).\n",
      "\n",
      "**Methodology**\n",
      "\n",
      "The authors describe their approach, which involves:\n",
      "\n",
      "1. Pre-training a large Transformer model on a large programming corpus.\n",
      "2. Fine-tuning the pre-trained model for specific tasks such as code translation, completion, and problem-solving.\n",
      "3. Evaluating the models using various benchmarks.\n",
      "\n",
      "**Results**\n",
      "\n",
      "The authors present the results of their experiments, including:\n",
      "\n",
      "* Code generation: The authors achieve state-of-the-art performance on benchmarking tasks such as code translation (26.7% vs. 24.2%), completion (29.2% vs. 25.1%), and problem-solving (28.9% vs. 27.6%).\n",
      "* Controllable code generation: The authors report that their model can generate highly-commented codes, which is a key aspect of controllable code generation.\n",
      "\n",
      "**Discussion**\n",
      "\n",
      "The authors discuss the implications of their results, including:\n",
      "\n",
      "* The potential for large language models to tackle complex programming tasks.\n",
      "* The importance of controllable code generation in programming competitions and other applications.\n",
      "* The need for further research on developing more robust and generalizable models.\n",
      "\n",
      "Overall, the paper presents a compelling case for using large language models like Transformer for code generation and provides valuable insights into the state-of-the-art performance on benchmarking tasks.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize Ollama Chat Model\n",
    "chat_model = ChatOllama(\n",
    "    model=\"llama3.2:1b\",  # You can change to other models like mistral, phi3\n",
    "    temperature=0.7,  # Creativity level\n",
    "    num_gpu=16  # Optional GPU configuration\n",
    ")\n",
    "\n",
    "# Simple Chat Interaction\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant for my research.\"),\n",
    "    HumanMessage(content= f\"Query + {query} with the context + {contexts}\")\n",
    "]\n",
    "\n",
    "# Generate Response\n",
    "response = chat_model.invoke(messages)\n",
    "print(\"--- Ollama Response ---\")\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
