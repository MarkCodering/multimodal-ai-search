{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: pymilvus[model]\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pymilvus[model] langchain-milvus langchain-openai\n",
    "%pip install -qU langchain_community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTION_URI = \"http://localhost:19530\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_embedding_func = OllamaLLM(model=\"moondream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2303.05510v1.pdf']\n",
      "Vector store for 2303.05510v1.pdf created successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Ollama Setup\n",
    "def setup_ollama_pdf_vectorstore(file_path, embedding_model=\"moondream\"):\n",
    "    try:\n",
    "        # 1. Load PDF\n",
    "        loader = PyMuPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "\n",
    "        # 2. Text Splitting (Optional but recommended)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        splits = text_splitter.split_documents(docs)\n",
    "\n",
    "        # 3. Ollama Embeddings\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            model=embedding_model,  # You can change this to other Ollama models\n",
    "            num_gpu=16  # Optional: specify GPU usage\n",
    "        )\n",
    "\n",
    "        # 4. Create Chroma Vector Store\n",
    "        vector_store = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embeddings,\n",
    "            collection_name=\"ollama_pdf_collection\"\n",
    "        )\n",
    "\n",
    "        return vector_store\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "files = os.listdir(\"papers\")\n",
    "print(files)\n",
    "\n",
    "vector_store = setup_ollama_pdf_vectorstore(f\"papers/{files[0]}\")\n",
    "print(f\"Vector store for {files[0]} created successfully\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Similarity Search\n",
    "contexts = \"\"\n",
    "query = input(\"Enter your query: \")\n",
    "\n",
    "if vector_store:\n",
    "    results = vector_store.similarity_search(query, k=10)\n",
    "    \n",
    "    for doc in results:\n",
    "        print(\"\\n--- Relevant Document Excerpt ---\")\n",
    "        print(doc.page_content)\n",
    "        contexts = contexts + doc.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ollama Response ---\n",
      "The text describes a software tool for generating code solutions to specific problems in an interview setting, called \"prophet\". Here's a breakdown of the key points:\n",
      "\n",
      "**Problem Statement**\n",
      "\n",
      "Prophet aims to generate solutions to problems from standard input (integer values) using a combination of natural language and mathematical reasoning. The goal is to produce solutions that meet certain constraints, such as passing certain tests or having an integer output.\n",
      "\n",
      "**Input/Output**\n",
      "\n",
      "* Input: Integer values on standard input\n",
      "* Output: String representations of generated code solutions with specific formatting\n",
      "\n",
      "**Features**\n",
      "\n",
      "* Optimized for performance using various techniques (e.g., beam search)\n",
      "* Can generate solutions from problem inputs in multiple formats (e.g., pass rate, strict accuracy)\n",
      "\n",
      "**Methodology**\n",
      "\n",
      "1. **Problem Definition**: Problem description and constraints are provided.\n",
      "2. **Input Processing**: Input values are processed to ensure they meet the specified constraints.\n",
      "3. **Code Generation**: A solution is generated using a combination of natural language and mathematical reasoning.\n",
      "4. **Post-processing**: Code solutions are formatted according to specific requirements.\n",
      "\n",
      "**Performance**\n",
      "\n",
      "* Results are reported in various tables, including:\n",
      "\t+ Performance metrics (e.g., pass rate, strict accuracy)\n",
      "\t+ Comparison with original solutions\n",
      "\t+ Comparison with other code generation tools\n",
      "\n",
      "**Improvements**\n",
      "\n",
      "* Fine-tuning of the model using Prophet-generated solutions\n",
      "* Evaluation of optimization techniques beyond pass rate\n",
      "\n",
      "**Objectives**\n",
      "\n",
      "Prophet aims to:\n",
      "\n",
      "1. **Improve Performance**: Enhance the tool's ability to generate high-quality solutions.\n",
      "2. **Increase Flexibility**: Introduce new reward functions and objectives to make the algorithm more versatile.\n",
      "\n",
      "The text concludes by highlighting the versatility of Prophet, with two new objective functions (code length penalty and comment encouragement) that can be used to improve its performance and flexibility.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize Ollama Chat Model\n",
    "chat_model = ChatOllama(\n",
    "    model=\"llama3.2:1b\",  # You can change to other models like mistral, phi3\n",
    "    temperature=0.7,  # Creativity level\n",
    "    num_gpu=16  # Optional GPU configuration\n",
    ")\n",
    "\n",
    "# Simple Chat Interaction\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant for my research.\"),\n",
    "    HumanMessage(content= f\"Query + {query} with the context + {contexts}\")\n",
    "]\n",
    "\n",
    "# Generate Response\n",
    "response = chat_model.invoke(messages)\n",
    "print(\"--- Ollama Response ---\")\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
