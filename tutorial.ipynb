{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: pymilvus[model]\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet pymilvus[model] langchain-milvus langchain-openai\n",
    "%pip install -qU langchain_community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTION_URI = \"http://localhost:19530\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_embedding_func = OllamaLLM(model=\"moondream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2303.05510v1.pdf']\n",
      "Vector store for 2303.05510v1.pdf created successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Ollama Setup\n",
    "def setup_ollama_pdf_vectorstore(file_path, embedding_model=\"moondream\"):\n",
    "    try:\n",
    "        # 1. Load PDF\n",
    "        loader = PyMuPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "\n",
    "        # 2. Text Splitting (Optional but recommended)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        splits = text_splitter.split_documents(docs)\n",
    "\n",
    "        # 3. Ollama Embeddings\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            model=embedding_model,  # You can change this to other Ollama models\n",
    "            num_gpu=16  # Optional: specify GPU usage\n",
    "        )\n",
    "\n",
    "        # 4. Create Chroma Vector Store\n",
    "        vector_store = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embeddings,\n",
    "            collection_name=\"ollama_pdf_collection\"\n",
    "        )\n",
    "\n",
    "        return vector_store\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "files = os.listdir(\"papers\")\n",
    "print(files)\n",
    "\n",
    "vector_store = setup_ollama_pdf_vectorstore(f\"papers/{files[0]}\")\n",
    "print(f\"Vector store for {files[0]} created successfully\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "Published as a conference paper at ICLR 2023\n",
      "APPENDIX\n",
      "In this appendix, we supplement the main paper by providing more thorough empirical evaluations to\n",
      "back up our claims and more detailed descriptions of the algorithms to help readers better understand\n",
      "our paper.\n",
      "This appendix is organized as follows.\n",
      "• In Sec. A, we provide more comprehensive results of our algorithm and the baseline algorithms.\n",
      "We also include the license information of the datasets we use.\n",
      "• In Sec. B, we consider the scenario where test cases are not provided. We evaluate our PG-TD\n",
      "algorithm using automatically-generated test cases.\n",
      "• In Sec. C, we provide empirical evidence for our claims in the discussion section that our\n",
      "algorithm is versatile and can be used to optimize different code generation objectives other\n",
      "than the pass rate. We consider the objectives of generating concise codes and generating\n",
      "codes with more comments.\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "Published as a conference paper at ICLR 2023\n",
      "APPENDIX\n",
      "In this appendix, we supplement the main paper by providing more thorough empirical evaluations to\n",
      "back up our claims and more detailed descriptions of the algorithms to help readers better understand\n",
      "our paper.\n",
      "This appendix is organized as follows.\n",
      "• In Sec. A, we provide more comprehensive results of our algorithm and the baseline algorithms.\n",
      "We also include the license information of the datasets we use.\n",
      "• In Sec. B, we consider the scenario where test cases are not provided. We evaluate our PG-TD\n",
      "algorithm using automatically-generated test cases.\n",
      "• In Sec. C, we provide empirical evidence for our claims in the discussion section that our\n",
      "algorithm is versatile and can be used to optimize different code generation objectives other\n",
      "than the pass rate. We consider the objectives of generating concise codes and generating\n",
      "codes with more comments.\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "Published as a conference paper at ICLR 2023\n",
      "Pass Rate (%)\n",
      "Strict Accuracy (%)\n",
      "APPS Intro.\n",
      "APPS Inter.\n",
      "APPS comp.\n",
      "CodeContests\n",
      "APPS Intro.\n",
      "APPS Inter.\n",
      "APPS comp.\n",
      "CodeContests\n",
      "APPS GPT-2\n",
      "Beam Search\n",
      "11.95\n",
      "9.55\n",
      "5.04\n",
      "5.10\n",
      "5.50\n",
      "2.10\n",
      "1.00\n",
      "0.00\n",
      "Sampling+Filtering\n",
      "25.19\n",
      "24.13\n",
      "11.92\n",
      "20.40\n",
      "13.80\n",
      "5.70\n",
      "2.30\n",
      "3.64\n",
      "SMCG-TD\n",
      "24.10\n",
      "21.98\n",
      "10.37\n",
      "17.47\n",
      "11.70\n",
      "5.50\n",
      "2.10\n",
      "4.24\n",
      "PG-TD (c = 4)\n",
      "26.70\n",
      "24.92\n",
      "12.89\n",
      "24.05\n",
      "13.10\n",
      "6.10\n",
      "3.10\n",
      "4.85\n",
      "APPS GPT-Neo\n",
      "Beam Search\n",
      "14.32\n",
      "9.80\n",
      "6.39\n",
      "5.73\n",
      "6.70\n",
      "2.00\n",
      "2.10\n",
      "0.00\n",
      "Sampling+Filtering\n",
      "27.71\n",
      "24.85\n",
      "12.55\n",
      "25.26\n",
      "15.50\n",
      "5.80\n",
      "3.00\n",
      "4.24\n",
      "SMCG-TD\n",
      "25.09\n",
      "20.34\n",
      "9.16\n",
      "15.44\n",
      "13.80\n",
      "5.10\n",
      "1.80\n",
      "3.03\n",
      "PG-TD (c = 4)\n",
      "29.27\n",
      "25.69\n",
      "13.55\n",
      "26.07\n",
      "15.50\n",
      "6.43\n",
      "3.50\n",
      "4.85\n",
      "Table 1: Results of PG-TD and other algorithms. The maximum number of Transformer generations\n",
      "for all algorithms is 256.\n",
      "200\n",
      "400\n",
      "600\n",
      "# of Transformer Generations\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "Pass Rate (%)\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "Computation Time (sec.)\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "Pass Rate (%)\n",
      "SMCG-TD\n",
      "Sampling + Filtering\n",
      "PG-TD (c = 2)\n",
      "PG-TD (c = 4)\n",
      "PG-TD (c = 6)\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "Published as a conference paper at ICLR 2023\n",
      "Pass Rate (%)\n",
      "Strict Accuracy (%)\n",
      "APPS Intro.\n",
      "APPS Inter.\n",
      "APPS comp.\n",
      "CodeContests\n",
      "APPS Intro.\n",
      "APPS Inter.\n",
      "APPS comp.\n",
      "CodeContests\n",
      "APPS GPT-2\n",
      "Beam Search\n",
      "11.95\n",
      "9.55\n",
      "5.04\n",
      "5.10\n",
      "5.50\n",
      "2.10\n",
      "1.00\n",
      "0.00\n",
      "Sampling+Filtering\n",
      "25.19\n",
      "24.13\n",
      "11.92\n",
      "20.40\n",
      "13.80\n",
      "5.70\n",
      "2.30\n",
      "3.64\n",
      "SMCG-TD\n",
      "24.10\n",
      "21.98\n",
      "10.37\n",
      "17.47\n",
      "11.70\n",
      "5.50\n",
      "2.10\n",
      "4.24\n",
      "PG-TD (c = 4)\n",
      "26.70\n",
      "24.92\n",
      "12.89\n",
      "24.05\n",
      "13.10\n",
      "6.10\n",
      "3.10\n",
      "4.85\n",
      "APPS GPT-Neo\n",
      "Beam Search\n",
      "14.32\n",
      "9.80\n",
      "6.39\n",
      "5.73\n",
      "6.70\n",
      "2.00\n",
      "2.10\n",
      "0.00\n",
      "Sampling+Filtering\n",
      "27.71\n",
      "24.85\n",
      "12.55\n",
      "25.26\n",
      "15.50\n",
      "5.80\n",
      "3.00\n",
      "4.24\n",
      "SMCG-TD\n",
      "25.09\n",
      "20.34\n",
      "9.16\n",
      "15.44\n",
      "13.80\n",
      "5.10\n",
      "1.80\n",
      "3.03\n",
      "PG-TD (c = 4)\n",
      "29.27\n",
      "25.69\n",
      "13.55\n",
      "26.07\n",
      "15.50\n",
      "6.43\n",
      "3.50\n",
      "4.85\n",
      "Table 1: Results of PG-TD and other algorithms. The maximum number of Transformer generations\n",
      "for all algorithms is 256.\n",
      "200\n",
      "400\n",
      "600\n",
      "# of Transformer Generations\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "Pass Rate (%)\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "Computation Time (sec.)\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "Pass Rate (%)\n",
      "SMCG-TD\n",
      "Sampling + Filtering\n",
      "PG-TD (c = 2)\n",
      "PG-TD (c = 4)\n",
      "PG-TD (c = 6)\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "Published as a conference paper at ICLR 2023\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "# of Transformer Generations\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "Pass Rate (%)\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "Computation Time (sec.)\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "Pass Rate (%)\n",
      "SMCG-TD\n",
      "Sampling + Filtering\n",
      "PG-TD (b = 1)\n",
      "PG-TD (b = 3)\n",
      "PG-TD (b = 5)\n",
      "Figure 5: Results of PG-TD (c = 2) on the APPS introductory dataset, using the APPS GPT-2\n",
      "Transformer model.\n",
      "200\n",
      "400\n",
      "# of Transformer Generations\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "Pass Rate (%)\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "Computation Time (sec.)\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "Pass Rate (%)\n",
      "SMCG-TD\n",
      "Sampling + Filtering\n",
      "PG-TD (b = 1)\n",
      "PG-TD (b = 3)\n",
      "PG-TD (b = 5)\n",
      "Figure 6: Results of PG-TD (c = 4) on the APPS introductory dataset, using the APPS GPT-2\n",
      "Transformer model.\n",
      "200\n",
      "400\n",
      "# of Transformer Generations\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "Pass Rate (%)\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "Computation Time (sec.)\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "Pass Rate (%)\n",
      "PG-TD (k = 2)\n",
      "PG-TD (k = 3)\n",
      "PG-TD (k = 4)\n",
      "Figure 7: Results of PG-TD (c = 4) with different k on the APPS introductory dataset, using the\n",
      "APPS GPT-2 Transformer model..\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "Published as a conference paper at ICLR 2023\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "# of Transformer Generations\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "Pass Rate (%)\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "Computation Time (sec.)\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "Pass Rate (%)\n",
      "SMCG-TD\n",
      "Sampling + Filtering\n",
      "PG-TD (b = 1)\n",
      "PG-TD (b = 3)\n",
      "PG-TD (b = 5)\n",
      "Figure 5: Results of PG-TD (c = 2) on the APPS introductory dataset, using the APPS GPT-2\n",
      "Transformer model.\n",
      "200\n",
      "400\n",
      "# of Transformer Generations\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "Pass Rate (%)\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "Computation Time (sec.)\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "Pass Rate (%)\n",
      "SMCG-TD\n",
      "Sampling + Filtering\n",
      "PG-TD (b = 1)\n",
      "PG-TD (b = 3)\n",
      "PG-TD (b = 5)\n",
      "Figure 6: Results of PG-TD (c = 4) on the APPS introductory dataset, using the APPS GPT-2\n",
      "Transformer model.\n",
      "200\n",
      "400\n",
      "# of Transformer Generations\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "Pass Rate (%)\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "Computation Time (sec.)\n",
      "22\n",
      "24\n",
      "26\n",
      "28\n",
      "Pass Rate (%)\n",
      "PG-TD (k = 2)\n",
      "PG-TD (k = 3)\n",
      "PG-TD (k = 4)\n",
      "Figure 7: Results of PG-TD (c = 4) with different k on the APPS introductory dataset, using the\n",
      "APPS GPT-2 Transformer model..\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "test cases to verify the generated programs.\n",
      "C\n",
      "PLANNING FOR OTHER CODE GENERATION OBJECTIVES\n",
      "Besides the default reward function that optimizes the pass rate, we show the versatility of the pro-\n",
      "posed algorithm by setting two new objectives, code length penalty and comment encouragement.\n",
      "Code length penalty. We make the planner generate more concise codes by using the following\n",
      "reward function\n",
      "Rlength = p + λ1 × e−lc/t,\n",
      "(1)\n",
      "where p is the average pass rate on the public test case set and lc is the length of the code string. λ\n",
      "and t are hyperparameters that control the weight of the code length penalty and are set to 0.1 and\n",
      "20, respectively. As shown in Figure 9, the generated solution becomes more concise and the code\n",
      "string length decreases from 187 to 78 while still passing all the test cases.\n",
      "Comment encouragement. We can also generate solutions with more comments. We achieve this\n",
      "goal by using the following reward function\n",
      "Rcomment = p + λ1 × e−lc/t + λ2 × min(1, Ncm\n",
      "Nmax\n",
      "),\n",
      "(2)\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "test cases to verify the generated programs.\n",
      "C\n",
      "PLANNING FOR OTHER CODE GENERATION OBJECTIVES\n",
      "Besides the default reward function that optimizes the pass rate, we show the versatility of the pro-\n",
      "posed algorithm by setting two new objectives, code length penalty and comment encouragement.\n",
      "Code length penalty. We make the planner generate more concise codes by using the following\n",
      "reward function\n",
      "Rlength = p + λ1 × e−lc/t,\n",
      "(1)\n",
      "where p is the average pass rate on the public test case set and lc is the length of the code string. λ\n",
      "and t are hyperparameters that control the weight of the code length penalty and are set to 0.1 and\n",
      "20, respectively. As shown in Figure 9, the generated solution becomes more concise and the code\n",
      "string length decreases from 187 to 78 while still passing all the test cases.\n",
      "Comment encouragement. We can also generate solutions with more comments. We achieve this\n",
      "goal by using the following reward function\n",
      "Rcomment = p + λ1 × e−lc/t + λ2 × min(1, Ncm\n",
      "Nmax\n",
      "),\n",
      "(2)\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "Published as a conference paper at ICLR 2023\n",
      "Problem Statement\n",
      "Given is an integer r. How many times is the area of a circle of radius r larger than the area\n",
      "of a circle of radius 1? It can be proved that the answer is always an integer under the constraints\n",
      "given.\n",
      "Constraints\n",
      "(1). 1 ≤r ≤100.\n",
      "(2). All values in input are integers.\n",
      "Input\n",
      "Input is given from Standard Input in the following format: r.\n",
      "Output\n",
      "Print the area of a circle of radius r, divided by the area of a circle of radius 1, as an integer..\n",
      "Sample Test Input\n",
      "2\n",
      "Sample Test Output\n",
      "4\n",
      "The area of a circle of radius 2 is 4 times larger than the area of a circle of radius 1. Note that\n",
      "output must be an integer - for example, 4.0 will not be accepted.\n",
      "1\n",
      "# cook your dish here\n",
      "2\n",
      "r=int(input())\n",
      "3\n",
      "if(r>=2*r):\n",
      "4\n",
      "print(r*r*2)\n",
      "5\n",
      "else:\n",
      "6\n",
      "print(0)\n",
      "Beam Search (Pass Rate: 0.00).\n",
      "1\n",
      "# cook your dish here\n",
      "2\n",
      "r=int(input())\n",
      "3\n",
      "if(r>=2*r):\n",
      "4\n",
      "print(r*r*2)\n",
      "5\n",
      "else:\n",
      "6\n",
      "print(0)\n",
      "Sampling + Filtering (Pass Rate:\n",
      "0.00).\n",
      "1\n",
      "# cook your dish here\n",
      "2\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "Transformer model, we run beam search to generate solutions on the test set to see if it has a better\n",
      "performance. We use the ﬁrst 500 problems in the interview-level problems in APPS test set for\n",
      "validation and the introductory-level problems in APPS test set for testing. The results are reported\n",
      "in Table 3. After ﬁnetuning with PG-TD-generated solutions, we see improvement in both pass rate\n",
      "and strict accuracy. More details are in Sec. D.4.\n",
      "Methods\n",
      "Code ↓\n",
      "Comment ↑\n",
      "Pass ↑\n",
      "length\n",
      "number\n",
      "rate\n",
      "Default\n",
      "248.42\n",
      "0.68\n",
      "23.14\n",
      "Length Penalty\n",
      "190.73\n",
      "-\n",
      "22.82\n",
      "Comment Encouragement\n",
      "-\n",
      "3.11\n",
      "21.65\n",
      "Table 4: Performance of controllable code generation.\n",
      "Code length denotes the length of the generated code\n",
      "string and comment number denotes the number of code\n",
      "lines that contains comments.\n",
      "Optimizing other code generation\n",
      "objectives.\n",
      "Beyond the pass rate,\n",
      "we can make the algorithm versatile\n",
      "by using different reward functions.\n",
      "We consider two new objectives, code\n",
      "length penalty and comment encour-\n",
      "agement.\n"
     ]
    }
   ],
   "source": [
    "# Perform Similarity Search\n",
    "contexts = \"\"\n",
    "query = input(\"Enter your query: \")\n",
    "\n",
    "if vector_store:\n",
    "    results = vector_store.similarity_search(query, k=10)\n",
    "    \n",
    "    for doc in results:\n",
    "        print(\"\\n--- Relevant Document Excerpt ---\")\n",
    "        print(doc.page_content)\n",
    "        contexts = contexts + doc.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ollama Response ---\n",
      "The text appears to be a research paper on a code generation algorithm that aims to optimize various code generation objectives. Here's a breakdown of the key points:\n",
      "\n",
      "**Problem Statement**\n",
      "\n",
      "The algorithm is designed to solve the problem of generating codes that meet certain criteria, such as passing multiple test cases and having an integer area ratio.\n",
      "\n",
      "**Algorithm Overview**\n",
      "\n",
      "The algorithm uses a combination of algorithms to generate solutions. It includes:\n",
      "\n",
      "1. **Beam Search**: A beam search algorithm is used to search through the code space and find optimal solutions.\n",
      "2. **PLANNING FOR OTHER CODE GENERATION OBJECTIVES**: The algorithm can be configured to optimize various objectives, such as pass rate, length penalty, comment encouragement, or a combination of these.\n",
      "\n",
      "**Reward Functions**\n",
      "\n",
      "The algorithm uses two reward functions:\n",
      "\n",
      "1. **Default Reward Function**: The default reward function optimizes the pass rate.\n",
      "2. **Code Length Penalty and Comment Encouragement**: This set of reward functions optimizes other objectives by penalizing longer code strings and encouraging more comments.\n",
      "\n",
      "**Code Generation**\n",
      "\n",
      "The algorithm generates solutions using a combination of algorithms, including:\n",
      "\n",
      "1. **Random Code Generation**: Random code generation is used as a baseline for comparison.\n",
      "2. **Pass Rate Optimization**: The pass rate is optimized using the default reward function.\n",
      "3. **Length Penalty Optimization**: The length penalty is optimized using the code length penalty reward function.\n",
      "4. **Comment Encouragement Optimization**: The comment encouragement reward function optimizes the number of comments in the generated solutions.\n",
      "\n",
      "**Evaluation**\n",
      "\n",
      "The algorithm is evaluated on various test sets, including:\n",
      "\n",
      "1. **Pass Rate**: The pass rate of the algorithm is compared to that of a baseline solution.\n",
      "2. **Strict Accuracy**: The strict accuracy of the algorithm is also evaluated using a set of test cases.\n",
      "3. **Code Length Ratio**: The area ratio of the generated solutions is calculated and reported.\n",
      "\n",
      "**Results**\n",
      "\n",
      "The results show that the algorithm improves in both pass rate and strict accuracy after fien-tuning with PG-TD-generated solutions. Additionally, the algorithm can be configured to optimize other objectives by using different reward functions.\n",
      "\n",
      "Overall, the algorithm appears to be a versatile code generation system that can be tailored to meet specific requirements by adjusting various objective weights.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize Ollama Chat Model\n",
    "chat_model = ChatOllama(\n",
    "    model=\"llama3.2:1b\",  # You can change to other models like mistral, phi3\n",
    "    temperature=0.7,  # Creativity level\n",
    "    num_gpu=16  # Optional GPU configuration\n",
    ")\n",
    "\n",
    "# Simple Chat Interaction\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant for my research.\"),\n",
    "    HumanMessage(content= f\"Query + {query} with the context + {contexts}\")\n",
    "]\n",
    "\n",
    "# Generate Response\n",
    "response = chat_model.invoke(messages)\n",
    "print(\"--- Ollama Response ---\")\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
