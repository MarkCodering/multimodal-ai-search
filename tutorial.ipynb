{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-milvus in ./venv/lib/python3.12/site-packages (0.1.8)\n",
      "Requirement already satisfied: langchain-openai in ./venv/lib/python3.12/site-packages (0.3.4)\n",
      "Requirement already satisfied: pymilvus[model] in ./venv/lib/python3.12/site-packages (2.5.4)\n",
      "Requirement already satisfied: setuptools>69 in ./venv/lib/python3.12/site-packages (from pymilvus[model]) (75.8.0)\n",
      "Requirement already satisfied: grpcio<=1.67.1,>=1.49.1 in ./venv/lib/python3.12/site-packages (from pymilvus[model]) (1.67.1)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in ./venv/lib/python3.12/site-packages (from pymilvus[model]) (5.29.3)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in ./venv/lib/python3.12/site-packages (from pymilvus[model]) (1.0.1)\n",
      "Requirement already satisfied: ujson>=2.0.0 in ./venv/lib/python3.12/site-packages (from pymilvus[model]) (5.10.0)\n",
      "Requirement already satisfied: pandas>=1.2.4 in ./venv/lib/python3.12/site-packages (from pymilvus[model]) (2.2.3)\n",
      "Requirement already satisfied: milvus-lite>=2.4.0 in ./venv/lib/python3.12/site-packages (from pymilvus[model]) (2.4.11)\n",
      "Requirement already satisfied: milvus-model>=0.1.0 in ./venv/lib/python3.12/site-packages (from pymilvus[model]) (0.2.12)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.2.38 in ./venv/lib/python3.12/site-packages (from langchain-milvus) (0.3.34)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in ./venv/lib/python3.12/site-packages (from langchain-openai) (1.61.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in ./venv/lib/python3.12/site-packages (from langchain-openai) (0.8.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./venv/lib/python3.12/site-packages (from langchain-core<0.4,>=0.2.38->langchain-milvus) (0.3.7)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./venv/lib/python3.12/site-packages (from langchain-core<0.4,>=0.2.38->langchain-milvus) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./venv/lib/python3.12/site-packages (from langchain-core<0.4,>=0.2.38->langchain-milvus) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./venv/lib/python3.12/site-packages (from langchain-core<0.4,>=0.2.38->langchain-milvus) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./venv/lib/python3.12/site-packages (from langchain-core<0.4,>=0.2.38->langchain-milvus) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./venv/lib/python3.12/site-packages (from langchain-core<0.4,>=0.2.38->langchain-milvus) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in ./venv/lib/python3.12/site-packages (from langchain-core<0.4,>=0.2.38->langchain-milvus) (2.10.6)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.12/site-packages (from milvus-lite>=2.4.0->pymilvus[model]) (4.67.1)\n",
      "Requirement already satisfied: transformers>=4.36.0 in ./venv/lib/python3.12/site-packages (from milvus-model>=0.1.0->pymilvus[model]) (4.48.3)\n",
      "Requirement already satisfied: onnxruntime in ./venv/lib/python3.12/site-packages (from milvus-model>=0.1.0->pymilvus[model]) (1.20.1)\n",
      "Requirement already satisfied: scipy>=1.10.0 in ./venv/lib/python3.12/site-packages (from milvus-model>=0.1.0->pymilvus[model]) (1.15.1)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from milvus-model>=0.1.0->pymilvus[model]) (1.26.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.12/site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus[model]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus[model]) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus[model]) (2025.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./venv/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./venv/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
      "Requirement already satisfied: idna>=2.8 in ./venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai) (3.10)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.2.38->langchain-milvus) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langchain-milvus) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langchain-milvus) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.2.38->langchain-milvus) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.38->langchain-milvus) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.2.38->langchain-milvus) (2.27.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus[model]) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in ./venv/lib/python3.12/site-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (0.28.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./venv/lib/python3.12/site-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./venv/lib/python3.12/site-packages (from transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (0.5.2)\n",
      "Requirement already satisfied: coloredlogs in ./venv/lib/python3.12/site-packages (from onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./venv/lib/python3.12/site-packages (from onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (25.1.24)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.12/site-packages (from onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (1.13.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers>=4.36.0->milvus-model>=0.1.0->pymilvus[model]) (2025.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./venv/lib/python3.12/site-packages (from coloredlogs->onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy->onnxruntime->milvus-model>=0.1.0->pymilvus[model]) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pymilvus[model] langchain-milvus langchain-openai\n",
    "%pip install -qU langchain_community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTION_URI = \"http://localhost:19530\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_embedding_func = OllamaLLM(model=\"moondream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2303.05510v1.pdf']\n",
      "Vector store for 2303.05510v1.pdf created successfully\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Ollama Setup\n",
    "def setup_ollama_pdf_vectorstore(file_path, embedding_model=\"moondream\"):\n",
    "    try:\n",
    "        # 1. Load PDF\n",
    "        loader = PyMuPDFLoader(file_path)\n",
    "        docs = loader.load()\n",
    "\n",
    "        # 2. Text Splitting (Optional but recommended)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        splits = text_splitter.split_documents(docs)\n",
    "\n",
    "        # 3. Ollama Embeddings\n",
    "        embeddings = OllamaEmbeddings(\n",
    "            model=embedding_model,  # You can change this to other Ollama models\n",
    "            num_gpu=6  # Optional: specify GPU usage\n",
    "        )\n",
    "\n",
    "        # 4. Create Chroma Vector Store\n",
    "        vector_store = Chroma.from_documents(\n",
    "            documents=splits,\n",
    "            embedding=embeddings,\n",
    "            collection_name=\"ollama_pdf_collection\"\n",
    "        )\n",
    "\n",
    "        return vector_store\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "files = os.listdir(\"papers\")\n",
    "print(files)\n",
    "\n",
    "vector_store = setup_ollama_pdf_vectorstore(f\"papers/{files[0]}\")\n",
    "print(f\"Vector store for {files[0]} created successfully\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "Published as a conference paper at ICLR 2023\n",
      "Problem Statement\n",
      "You are given an array a consisting of n integer numbers.\n",
      "Let instability of the array be the following value:\n",
      "n\n",
      "max\n",
      "i=1 ai −\n",
      "n\n",
      "min\n",
      "i=1 ai.\n",
      "You have to remove exactly one element from this array to minimize instability of the resulting\n",
      "(n −1)-elements array. Your task is to calculate the minimum possible instability.\n",
      "Input\n",
      "The ﬁrst line of the input contains one integer n (2 ≤n ≤105) — the number of elements in the\n",
      "array a.\n",
      "The second line of the input contains n integers a1, a2, . . . , an (1 ≤ai ≤105) — elements of the\n",
      "array a.\n",
      "Output\n",
      "Print one integer — the minimum possible instability of the array if you have to remove exactly one\n",
      "element from the array a.\n",
      "Examples\n",
      "Input\n",
      "4\n",
      "1 3 3 7\n",
      "Output\n",
      "2\n",
      "Input\n",
      "2\n",
      "1 100000\n",
      "Output\n",
      "0\n",
      "Note\n",
      "In the ﬁrst example you can remove 7 then instability of the remaining array will be 3 −1 = 2.\n",
      "In the second example you can remove either 1 or 100000 then instability of the remaining array\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "a tree search algorithm inspired by Monte-Carlo tree search. We evaluate our algorithm and em-\n",
      "pirically show that our algorithm generates programs of higher quality compared with competing\n",
      "baseline algorithms in different settings. We also design model structure-speciﬁc caching mecha-\n",
      "nisms which contribute to saving computational expenses. We show that our algorithm is versatile\n",
      "and can generate codes under objectives other than the pass rate without ﬁnetuning the Transformer.\n",
      "We hope our work can inspire more ways of incorporating planning algorithms into the Transformer\n",
      "generation process for code generation or other problems.\n",
      "One limitation of our algorithm is its reliance on test cases, although we ﬁnd that even a small\n",
      "number of test cases can help ﬁnd better solutions (Sec. A, Table 7). To address this limitation, we\n",
      "provided results to show that our algorithm can take advantage of automatically-generated test cases\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "than the pass rate. We consider the objectives of generating concise codes and generating\n",
      "codes with more comments.\n",
      "• In Sec. D, we provide more details on the components in PG-TD as well as the baseline algo-\n",
      "rithms.\n",
      "• In Sec. E, we illustrate more examples of the codes generated by our PG-TD algorithm and the\n",
      "baseline algorithms.\n",
      "• In Sec. F, we discuss more on the advantages and the potential negative social impacts of our\n",
      "algorithm.\n",
      "A\n",
      "EMPIRICAL EVALUATION\n",
      "We reported the performance of our algorithm and other baselines on the whole APPS dataset\n",
      "(Hendrycks et al., 2021) and CodeContests (Li et al., 2022). The APPS test dataset contains coding\n",
      "problems at three levels: introductory (1000 problems), interview (3000 problems), and competition\n",
      "(1000 problems).\n",
      "In addition to our results in Table 1 in the main paper, Table 5 shows the results where the budget\n",
      "of the number of Transformer generations is 512. As we expect, with a larger number of generated\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "Published as a conference paper at ICLR 2023\n",
      "ab\n",
      "b\n",
      "<PD>\n",
      "a\n",
      "b\n",
      "aa\n",
      "ba\n",
      "a\n",
      "b\n",
      "a\n",
      "a\n",
      "<PD>\n",
      "a\n",
      "b\n",
      "aa\n",
      "ba\n",
      "a\n",
      "b\n",
      "a\n",
      "a\n",
      "Selection\n",
      "Expansion\n",
      "Evaluation\n",
      "Backpropagation\n",
      "ab\n",
      "b\n",
      "<PD>\n",
      "a\n",
      "b\n",
      "aa\n",
      "ba\n",
      "a\n",
      "b\n",
      "a\n",
      "a\n",
      "ab\n",
      "b\n",
      "<PD>\n",
      "a\n",
      "b\n",
      "aa\n",
      "ba\n",
      "a\n",
      "b\n",
      "a\n",
      "a\n",
      "Select the next \n",
      "action in the action \n",
      "set\n",
      "Generate a random policy\n",
      "and evaluate it\n",
      "Figure 11: The standard Monte-Carlo tree search algorithm, without using a pre-trained Trans-\n",
      "former. <PD> stands for problem description.\n",
      "a,\n",
      ",\n",
      "<PD>\n",
      "a\n",
      "x\n",
      "a=\n",
      "x,\n",
      "a\n",
      "x\n",
      "=\n",
      ",\n",
      "<PD>\n",
      "a\n",
      "x\n",
      "a=\n",
      "x,\n",
      "a\n",
      "x\n",
      "=\n",
      ",\n",
      "generates complete programs \n",
      "using beam search\n",
      "a,b\n",
      "a,\\n\n",
      "a,\n",
      ",\n",
      "<PD>\n",
      "a\n",
      "x\n",
      "a=\n",
      "x,\n",
      "a\n",
      "x\n",
      "=\n",
      ",\n",
      "a,\n",
      ",\n",
      "<PD>\n",
      "a\n",
      "x\n",
      "a=\n",
      "x,\n",
      "a\n",
      "x\n",
      "=\n",
      ",\n",
      "suggests next tokens\n",
      "…\n",
      "Selection\n",
      "Expansion\n",
      "Evaluation\n",
      "Backpropagation\n",
      "Transformer \n",
      "Evaluate on\n",
      "public test cases\n",
      "…\n",
      "Figure 12: Our proposed framework, PG-TD, repeated here for convenience.\n",
      "a,b\n",
      "a,c\n",
      "a,\n",
      "a,b,\n",
      "a,b=\n",
      "a,c\n",
      "a,c=\n",
      "a,c,\n",
      "…\n",
      "a,b\n",
      "a,\n",
      "a,b,\n",
      "a,b=\n",
      "…\n",
      "Beam 1\n",
      "Beam 2\n",
      "…\n",
      "…\n",
      "Beam 1\n",
      "Beam 2\n",
      "…\n",
      "…\n",
      "Figure 13: Illustration of an example where sequence caching may not return the correct sequence\n",
      "when b > 1.\n",
      "D.2\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "• First, we propose a novel algorithm, Planning-Guided Transformer Decoding (PG-TD), that\n",
      "uses a planning algorithm for lookahead search and guide the Transformer to generate better\n",
      "codes. Our algorithm is model-agnostic, which can work with any standard Transformer model,\n",
      "and does not require knowledge of the grammar of the generated programs.\n",
      "• Second, a direct integration of the planning algorithm with the Transformer decoding process\n",
      "can cause redundant uses of the Transformer beam search algorithm. We contribute to design-\n",
      "ing mechanisms that signiﬁcantly improve the computational efﬁciency of the algorithm.\n",
      "• Third, we evaluate our algorithm on competitive programming benchmarks and empirically\n",
      "show that our algorithm can consistently generate better programs in terms of the pass rate and\n",
      "other metrics compared with the baseline methods. We also show that our algorithm is versatile\n",
      "and can optimize objectives other than the pass rate for controllable code generation, such as\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "objectives.\n",
      "Beyond the pass rate,\n",
      "we can make the algorithm versatile\n",
      "by using different reward functions.\n",
      "We consider two new objectives, code\n",
      "length penalty and comment encour-\n",
      "agement.\n",
      "As shown in Table 4, the\n",
      "code length penalty reward function\n",
      "makes the model generate more con-\n",
      "cise codes; the comment encourage-\n",
      "ment reward function encourages mod-\n",
      "els to generate codes with more comments. Both objectives still achieve reasonable pass rates on\n",
      "the public test cases. We provide qualitative examples and more details of the reward functions in\n",
      "Sec. C of the appendix.\n",
      "Using automatically-generated test cases. The datasets we use in this paper both contain test\n",
      "cases that can be used to compute rewards for PG-TD. However, in reality, human-speciﬁed test\n",
      "cases are not always available. Recently, Chen et al. (2022) observe that Transformer pre-trained\n",
      "on code generation can also generate useful test cases by adding an assert keyword at the end of\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "Published as a conference paper at ICLR 2023\n",
      "PLANNING WITH LARGE LANGUAGE MODELS\n",
      "FOR CODE GENERATION\n",
      "Shun Zhang, Zhenfang Chen, Yikang Shen\n",
      "MIT-IBM Watson AI Lab\n",
      "Mingyu Ding\n",
      "The University of Hong Kong\n",
      "Joshua B. Tenenbaum\n",
      "MIT BCS, CBMM, CSAIL\n",
      "Chuang Gan\n",
      "UMass Amherst, MIT-IBM Watson AI Lab\n",
      "ABSTRACT\n",
      "Existing large language model-based code generation pipelines typically use beam\n",
      "search or sampling algorithms during the decoding process. Although the pro-\n",
      "grams they generate achieve high token-matching-based scores, they often fail to\n",
      "compile or generate incorrect outputs. The main reason is that conventional Trans-\n",
      "former decoding algorithms may not be the best choice for code generation. In\n",
      "this work, we propose a novel Transformer decoding algorithm, Planning-Guided\n",
      "Transformer Decoding (PG-TD), that uses a planning algorithm to do lookahead\n",
      "search and guide the Transformer to generate better programs. Speciﬁcally, in-\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "number of public test cases, PG-TD still generates programs similar to human-written programs\n",
      "without overﬁtting the pubic test cases.\n",
      "Results on Codex.\n",
      "To evaluate the effectiveness of our algorithms on more powerful code-\n",
      "generation Transformers, we run both beam search and PG-TD algorithms using Codex (Chen et al.,\n",
      "2021a) as the backbone. We follow the “best practices” provided in the Codex guide to put the\n",
      "problem description into a block comment in the beginning, and ask Codex to complete the rest.\n",
      "The prompt looks like the following.\n",
      "\"\"\"\n",
      "Python 3\n",
      "{problem description}\n",
      "\"\"\"\n",
      "15\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "the Transformer beam search algorithm with tree search.\n",
      "Consider the example in Fig. 3, which shows two iter-\n",
      "ations of PG-TD. In the evaluation step of the t-th iter-\n",
      "ation, the Transformer beam search algorithm implicitly\n",
      "builds a tree to ﬁnd the most likely sequences within a\n",
      "beam (Fig. 3 (left)). Because we only keep b partial pro-\n",
      "grams in the beam, it is a tree where only b nodes with the\n",
      "highest likelihood are expanded at each level (in the illus-\n",
      "tration, b = 2). Other nodes are dropped and no longer\n",
      "considered by the beam search algorithm. In the (t+1)-st\n",
      "iteration, if the tree search algorithm selects “a,”, such a\n",
      "state is already visited in the Transformer beam search in\n",
      "the t-th iteration. When the algorithm needs to ﬁnd the\n",
      "top-k most likely next tokens, such information is already obtained in the t-th iteration and can be\n",
      "reused without recomputation. In our implementation, we cache the tree structure generated by the\n",
      "\n",
      "--- Relevant Document Excerpt ---\n",
      "set of CodeContests (165 problems) (Li et al., 2022). We use the same metrics as in Hendrycks et al.\n",
      "(2021), which are pass rates and strict accuracies on the private test cases. Speciﬁcally, the pass\n",
      "rate is the average percentage of the private test cases that the generated programs pass over all the\n",
      "problems. The strict accuracy is the percentage of the problems where the generated programs pass\n",
      "all the private test cases.\n",
      "Effectiveness of PG-TD. To make a fair comparison, we evaluate the best programs found by the\n",
      "algorithms when they use the same number of Transformer generations. Speciﬁcally, the num-\n",
      "ber of Transformer generations is the number of function calls of BEAM SEARCH in PG-TD and\n",
      "SMCG-TD (when no cached sequences are found and used in sequence caching), and the number\n",
      "of sampling function calls in S+F.\n",
      "As shown in Table 1, PG-TD consistently outperforms all the other baselines on all the datasets\n"
     ]
    }
   ],
   "source": [
    "# Perform Similarity Search\n",
    "contexts = \"\"\n",
    "query = input(\"Enter your query: \")\n",
    "\n",
    "if vector_store:\n",
    "    results = vector_store.similarity_search(query, k=10)\n",
    "    \n",
    "    for doc in results:\n",
    "        print(\"\\n--- Relevant Document Excerpt ---\")\n",
    "        print(doc.page_content)\n",
    "        contexts = contexts + doc.page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Ollama Response ---\n",
      "The text appears to be an abstract or introduction to a research paper discussing two new approaches for code generation using large language models. Here's a breakdown of the main points:\n",
      "\n",
      "**Background**\n",
      "\n",
      "* Existing code generation pipelines often use beam search or sampling algorithms during decoding, but they may not always generate correct outputs.\n",
      "* Conventional transformer decoding algorithms may not be suitable for code generation.\n",
      "\n",
      "**Proposed Approach: Planning-Guided Transformer Decoding (PG-TD)**\n",
      "\n",
      "* PG-TD uses a planning algorithm to do lookahead search and guide the transformer decoder to generate better programs.\n",
      "* The algorithm is designed specifically for code generation tasks, taking into account the problem description in a block comment.\n",
      "* Two variants of PG-TD are proposed: PG-TD with cached sequences (PG-TD-Cached) and PG-TD without sequence caching (PG-TD-NoCache).\n",
      "\n",
      "**Evaluation**\n",
      "\n",
      "* A dataset of 165 problems from CodeContests is used to evaluate the effectiveness of each variant of PG-TD.\n",
      "* The pass rates and strict accuracies are measured on private test cases for all variants.\n",
      "* To make a fair comparison, the number of transformer generations and sampling function calls is evaluated.\n",
      "\n",
      "**Results**\n",
      "\n",
      "* PG-TD consistently outperforms all other baselines on all datasets, with no significant difference between the two variants (PG-TD-Cached and PG-TD-NoCache).\n",
      "* The pass rates for PG-TD are significantly higher than those for the baseline methods.\n",
      "* The strict accuracies of PG-TD are also improved over the baseline methods.\n",
      "\n",
      "**Future Work**\n",
      "\n",
      "* The proposed approaches can be extended to other code generation objectives, such as code length penalty and comment encouragement.\n",
      "* Further research is needed to investigate the effectiveness of PG-TD in more challenging datasets.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize Ollama Chat Model\n",
    "chat_model = ChatOllama(\n",
    "    model=\"llama3.2:1b\",  # You can change to other models like mistral, phi3\n",
    "    temperature=0.7,  # Creativity level\n",
    "    num_gpu=1  # Optional GPU configuration\n",
    ")\n",
    "\n",
    "# Simple Chat Interaction\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant for my research.\"),\n",
    "    HumanMessage(content= f\"Query + {query} with the context + {contexts}\")\n",
    "]\n",
    "\n",
    "# Generate Response\n",
    "response = chat_model.invoke(messages)\n",
    "print(\"--- Ollama Response ---\")\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
